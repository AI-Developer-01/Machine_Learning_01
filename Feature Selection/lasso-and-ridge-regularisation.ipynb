{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<h2><div style=\"font-family: Trebuchet MS; background-color: red; color: #FFFFFF; padding: 12px; line-height: 1.5;\">Lasso Regularisation</div> </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e8712157da18254275da8ebece1a9a02be042280"
   },
   "source": [
    "**Regularization** consists in adding a penalty on the different parameters of the model to reduce the freedom of the model. Hence, the model will be less likely to fit the noise of the training data and will improve the generalization abilities of the model.  **Lasso** or** L1 **has the property that is able to shrink some of the coefficients to zero. It adds penalty  equivalent to absolute value of the magnitude of coefficients. Therefore, that feature can be removed from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "ba9c8627b6e7a0dc69595465499820250fb33071"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Lasso,LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "59fa38478ddda2967d203172511188262a477b5e"
   },
   "source": [
    "I will be using **santander customer satisfaction** dataset since I have used the same dataset in  It will be easy to compare the scores of both feature selection methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "5cf72799dc7d4d0c739f72fd2149c4a65b33333b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 371)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the train dataset. It contain more then 76000 records. Lets load 10000 records only to make things fast.\n",
    "df=pd.read_csv(r'C:\\Users\\LENOVO\\Downloads\\Feature Selection\\data\\train.csv',nrows=10000)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "5e2d4aa3763fb87998de97087740b7381110388f"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "9ae1265bd32cdd6c18a41886355f7e46de55dac7"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "004665618a8aba4a534dbfcd87568e1f75b7b2e7"
   },
   "outputs": [],
   "source": [
    "# separate dataset into train and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df.drop(labels=['TARGET'], axis=1),df['TARGET'],test_size=0.3,random_state=0)\n",
    "#Filling null value with 0.\n",
    "X_train.fillna(0,inplace=True)\n",
    "X_test.fillna(0,inplace=True)\n",
    "#Shape of training set and test set.\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "a4d815e6eebc52f54e355eea95b86e25ed269eae"
   },
   "outputs": [],
   "source": [
    "# linear models benefit from feature scaling\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "cbd24ea316cc08b0ffc9f0a87183da630123a4fa"
   },
   "outputs": [],
   "source": [
    "#Lets do the model fitting and feature selection all in single line of code.\n",
    "#I will be using Logistic Regression model and select Lasso (l1 as) as a penalty\n",
    "#I will be using SelectFromModel object which select the features which are non zero.\n",
    "#C=1 (Inverse of regularization strength.Smaller values specify stronger regularization.)\n",
    "#penalty='l1' (Specify the norm used in the penalization.Here we are using Lasso.)\n",
    "sel=SelectFromModel(LogisticRegression(C=1,penalty='l1'))\n",
    "sel.fit(scaler.transform(X_train),Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "4f073142dfe7ad348d07a724e1bc01a102120dc8"
   },
   "outputs": [],
   "source": [
    "print('Total features-->',X_train.shape[1])\n",
    "print('Selected featurs-->',sum(sel.get_support()))\n",
    "print('Removed featurs-->',np.sum(sel.estimator_.coef_==0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1c295deab1e99af014cd2e3734938770366f5d26"
   },
   "source": [
    "As we can see, we have used Lasso regularisation to remove non important features from the dataset. If we compare Lasso regularisation with there we used Constant, Quasi-Constant, Duplicate and Correlation methods to remove the non important features but here we have done all those things in just two lines. Isn't that great? But we should keep in mind that increasing the penalisation will remove more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "ea567eab1c7db015dc084bf284d97c8e6432579a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a function to build random forests and compare performance in train and test set\n",
    "def RandomForest(X_train, X_test, y_train, y_test):\n",
    "    rf = RandomForestClassifier(n_estimators=200, random_state=1, max_depth=4)\n",
    "    rf.fit(X_train, y_train)\n",
    "    print('Train set')\n",
    "    pred = rf.predict_proba(X_train)\n",
    "    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "    print('Test set')\n",
    "    pred = rf.predict_proba(X_test)\n",
    "    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "70df59a93bb7f945af72df8333375718ed489c2d"
   },
   "outputs": [],
   "source": [
    "#Transforming the training set and test set.\n",
    "X_train_lasso=sel.transform(X_train)\n",
    "X_test_lasso=sel.transform(X_test)\n",
    "RandomForest(X_train_lasso,X_test_lasso,Y_train,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6c24d911e1138dd9a749fb4dca0534f31486d45b",
    "collapsed": true
   },
   "source": [
    "Now we will compare the result of Filter method and Lasso Regularisation. Please click [Filter Method](https://www.kaggle.com/raviprakash438/filter-method-feature-selection/notebook) to see the result. <br>\n",
    "Filter Method Score = 0.76199<br>\n",
    "Lasso Regularisation Score = 0.76250 <br>\n",
    "So, the result is almost similar. Therefore, for this dataset any of two approaches will give the similar result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8e2992098dbd09220e169cec44e26a9b514b0f4f"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "<h2><div style=\"font-family: Trebuchet MS; background-color: red; color: #FFFFFF; padding: 12px; line-height: 1.5;\">  Ridge Regularisation</div> </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ff522bbf48b79a38166d263aa07bf2d3eb0911ab",
    "collapsed": true
   },
   "source": [
    "**Ridge** or **L2** regression is the most commonly used method of regularization for the  problems which  do not have a unique solution. It adds penalty equivalent to square of the magnitude of coefficients.  Unlike L1 it don't srink some of the coefficients to zero. It srink the coefficients near to zero but it never by zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "0634598dff67db10cda64dec8be17371c19c6b31"
   },
   "outputs": [],
   "source": [
    "#Lets do the model fitting and feature selection all in single line of code.\n",
    "#I will be using Logistic Regression model and select Lasso (l1 as) as a penalty\n",
    "#I will be using SelectFromModel object which select the features which are non zero.\n",
    "#C=1 (Inverse of regularization strength.Smaller values specify stronger regularization.)\n",
    "#penalty='l2' (Specify the norm used in the penalization.Here we are using Ridge. It is a default penalty.)\n",
    "sfm=SelectFromModel(LogisticRegression(C=1,penalty='l2'))\n",
    "sfm.fit(scaler.transform(X_train),Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "76897ccd7274965f4d3a8f6a0466db4329e9124a"
   },
   "outputs": [],
   "source": [
    "print('Total features-->',X_train.shape[1])\n",
    "print('Selected featurs-->',sum(sfm.get_support()))\n",
    "print('Removed featurs-->',np.sum(sfm.estimator_.coef_==0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "072aa39380e24e53cc2310e54eea7409e31e39ab"
   },
   "source": [
    "As I told that L2 or Ridge regression will not srink the coefficient to zero but here we are able to see 86 features have zero coefficient. L2 have not srink the coefficient of the features to zero. Actually, these are constant features\n",
    "\n",
    "You will thinking on what basis the selected feature count is 107 out of 370. Actually it is selecting those coefficients whose absolute value is greater than absolute coefficient mean  as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "f4118afcbebe02afd0b2fa6cf68b0abc3f0981f4"
   },
   "outputs": [],
   "source": [
    "np.sum(np.abs(sfm.estimator_.coef_)>np.abs(sfm.estimator_.coef_).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "680e34af151ea1ca0efef24adba06c5f28ae5ff3"
   },
   "outputs": [],
   "source": [
    "#Transforming the training set and test set.\n",
    "X_train_l2=sel.transform(X_train)\n",
    "X_test_l2=sel.transform(X_test)\n",
    "RandomForest(X_train_l2,X_test_l2,Y_train,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "130c31161fbdc73412d927b7dda4c1cc02f3d784"
   },
   "source": [
    "Now we will compare the result of Filter method, Lasso Regularisation and Ridge Regularisation. Please click  to see the result. <br>\n",
    "Filter Method Score = 0.76199<br>\n",
    "Lasso Regularisation Score = 0.76250 <br>\n",
    "Ridge Regularisation Score = 0.76090 <br>\n",
    "So, the result is almost similar. Therefore, for this dataset any of three approaches will give the similar result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "698598d8543cc16912b645bfcc94e2eec6e1e9b8"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e29f302b9347a51b33d719032bbf09eb49948033"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a3204f28d9700b75225c1996ba022fafa68b8ba1",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
